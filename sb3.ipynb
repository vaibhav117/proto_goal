{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.envs.robotics import utils\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import  SAC, DDPG, TD3\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.her import   HerReplayBuffer\n",
    "# from stable_baselines3.td3.policies import MultiInputPolicy\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from fetch_image_env import FetchReachImageEnv\n",
    "import torch as th\n",
    "from torch import nn\n",
    "import utils\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3 import  SAC, DDPG, TD3\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "from stable_baselines3.her import   HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "from fetch_image_env import FetchReachImageEnv, make_sb3_env, make\n",
    "import gym\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from gym.wrappers import Monitor\n",
    "from video import VideoRecorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env=make_sb3_env(env_name=\"fetch_reach\", action_repeat=2, max_episode_steps=50, seed=10, fixed=False, reward_type=\"dense\")\n",
    "# env = Monitor(env, './video1', force=True)\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<ActionRepeat<FetchReachImageEnv instance>>>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "vec_env = make_vec_env(lambda: env, n_envs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vec_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DummyVecEnv' object has no attribute 'normalize_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-13f5341bed51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DummyVecEnv' object has no attribute 'normalize_obs'"
     ]
    }
   ],
   "source": [
    "vec_env.normalize_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_norm= VecNormalize(vec_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('achieved_goal',\n",
       "              array([[1.3418326 , 0.7491004 , 0.53472275]], dtype=float32)),\n",
       "             ('desired_goal',\n",
       "              array([[1.426043  , 0.75990236, 0.61896676]], dtype=float32)),\n",
       "             ('desired_goal_image',\n",
       "              array([[[[114, 114, 114, ..., 114, 114, 114],\n",
       "                       [185, 185, 220, ..., 114, 114, 114],\n",
       "                       [255, 255, 255, ..., 114, 114, 114],\n",
       "                       ...,\n",
       "                       [ 53,  53,  59, ...,  65,  65,  65],\n",
       "                       [ 59,  65,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]],\n",
       "              \n",
       "                      [[219, 219, 219, ..., 219, 219, 219],\n",
       "                       [237, 237, 246, ..., 219, 219, 219],\n",
       "                       [255, 255, 255, ..., 219, 219, 219],\n",
       "                       ...,\n",
       "                       [ 67,  66,  66, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]],\n",
       "              \n",
       "                      [[145, 145, 145, ..., 145, 145, 145],\n",
       "                       [200, 200, 228, ..., 145, 145, 145],\n",
       "                       [255, 255, 255, ..., 145, 145, 145],\n",
       "                       ...,\n",
       "                       [ 53,  53,  59, ...,  65,  65,  65],\n",
       "                       [ 59,  65,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]]]], dtype=uint8)),\n",
       "             ('image_observation',\n",
       "              array([[[[114, 114, 114, ..., 114, 114, 114],\n",
       "                       [185, 185, 220, ..., 114, 114, 114],\n",
       "                       [255, 255, 255, ..., 114, 114, 114],\n",
       "                       ...,\n",
       "                       [ 59,  53,  59, ...,  65,  65,  65],\n",
       "                       [ 59,  62,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]],\n",
       "              \n",
       "                      [[219, 219, 219, ..., 219, 219, 219],\n",
       "                       [237, 237, 246, ..., 219, 219, 219],\n",
       "                       [255, 255, 255, ..., 219, 219, 219],\n",
       "                       ...,\n",
       "                       [ 66,  66,  65, ...,  65,  65,  65],\n",
       "                       [ 66,  65,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]],\n",
       "              \n",
       "                      [[145, 145, 145, ..., 145, 145, 145],\n",
       "                       [200, 200, 228, ..., 145, 145, 145],\n",
       "                       [255, 255, 255, ..., 145, 145, 145],\n",
       "                       ...,\n",
       "                       [ 59,  53,  59, ...,  65,  65,  65],\n",
       "                       [ 59,  62,  65, ...,  65,  65,  65],\n",
       "                       [ 65,  65,  65, ...,  65,  65,  65]]]], dtype=uint8)),\n",
       "             ('observation',\n",
       "              array([[ 1.3418326e+00,  7.4910039e-01,  5.3472275e-01,  1.9780513e-04,\n",
       "                       7.1519302e-05,  7.7393297e-06,  5.5199283e-08, -2.4292744e-06,\n",
       "                       4.7332564e-06, -2.2845522e-06]], dtype=float32))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('achieved_goal',\n",
       "              array([[1.3418326 , 0.7491004 , 0.53472275]], dtype=float32)),\n",
       "             ('desired_goal',\n",
       "              array([[1.426043  , 0.75990236, 0.61896676]], dtype=float32)),\n",
       "             ('desired_goal_image',\n",
       "              array([[[[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]],\n",
       "              \n",
       "                      [[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]],\n",
       "              \n",
       "                      [[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]]]], dtype=float32)),\n",
       "             ('image_observation',\n",
       "              array([[[[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]],\n",
       "              \n",
       "                      [[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]],\n",
       "              \n",
       "                      [[10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       ...,\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.],\n",
       "                       [10., 10., 10., ..., 10., 10., 10.]]]], dtype=float32)),\n",
       "             ('observation',\n",
       "              array([[ 1.3418326e+00,  7.4910039e-01,  5.3472275e-01,  1.9780513e-04,\n",
       "                       7.1519302e-05,  7.7393297e-06,  5.5199283e-08, -2.4292744e-06,\n",
       "                       4.7332564e-06, -2.2845522e-06]], dtype=float32))])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_norm.normalize_obs(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import (\n",
    "    DummyVecEnv,\n",
    "    VecEnv,\n",
    "    VecNormalize,\n",
    "    VecTransposeImage,\n",
    "    is_vecenv_wrapped,\n",
    "    unwrap_vec_normalize,\n",
    ")\n",
    "\n",
    "uvn = unwrap_vec_normalize(env)\n",
    "# uvn.get_original_obs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwrap_vec_normalize(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym import spaces\n",
    "isinstance(env.observation_space, spaces.Dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/sandipan/rl/research/proto_goal'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sandipan/miniconda3/envs/pad/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "monitor_dir=os.path.join(os.getcwd(), \"new_env\")\n",
    "env = Monitor(env, monitor_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0, 255, (3, 84, 84), uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.spaces[\"image_observation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TimeLimit<ActionRepeat<FetchReachImageEnv instance>>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Monitor' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-d8ceb1bb4a25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSubprocVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVecEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvec_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDummyVecEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/rl/research/stable-baselines3/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_fns)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menv_fns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mVecEnv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Monitor' object is not iterable"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv, VecEnv\n",
    "vec_env = DummyVecEnv(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoalReplayBuffer:\n",
    "\n",
    "    # check this out \n",
    "    # https://github.com/vitchyr/rlkit/blob/v0.1.2/rlkit/data_management/obs_dict_replay_buffer.py\n",
    "\n",
    "    def __init__(self, buffer_size, sample_func, env_params):\n",
    "        self.env_params= env_params\n",
    "        self.T= env_params[\"max_env_steps\"]\n",
    "        self.size= buffer_size // self.T\n",
    "        self.current_size = 0\n",
    "        self.n_transitions_stored = 0\n",
    "        self.sample_func = sample_func\n",
    "        # see observation_spec for dimensions\n",
    "        self.episode = defaultdict(list)\n",
    "        self.buffers = {'observation': np.empty([self.size, self.T , *self.env_params['observation']], dtype=np.float32),\n",
    "                'achieved_goal': np.empty([self.size, self.T, *self.env_params['achieved_goal']], dtype=np.float32),\n",
    "                'achieved_goal_image':np.empty([self.size, self.T , *self.env_params['achieved_goal_image']], dtype=np.uint8),\n",
    "                'achieved_goal_image_next':np.empty([self.size, self.T , *self.env_params['achieved_goal_image']], dtype=np.uint8),\n",
    "                'desired_goal': np.empty([self.size, self.T, *self.env_params['desired_goal']],dtype=np.float32),\n",
    "                'desired_goal_image':np.empty([self.size, self.T, *self.env_params['desired_goal_image']],dtype=np.uint8),\n",
    "                'actions': np.empty([self.size, self.T, *self.env_params['actions']], dtype=np.float32)\n",
    "                \n",
    "                }\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        temp_buffers={}\n",
    "        for key in self.buffers.keys():\n",
    "            temp_buffers[key]=self.buffers[key][:self.current_size]\n",
    "        # temp_buffers['achieved_goal_next'] = temp_buffers['achieved_goal'][:, 1:, :]\n",
    "        # temp_buffers['achieved_goal_image_next']=temp_buffers['achieved_goal_image'][:, 1:, :]\n",
    "\n",
    "        # sample transitions\n",
    "        transitions = self.sample_func(temp_buffers, batch_size)\n",
    "        \n",
    "        # discounts = np.ones((batch_size, 1), dtype=np.float32) * discount\n",
    "        # discounts = torch.as_tensor(discounts, device=self.device)\n",
    "        return transitions\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_size\n",
    "\n",
    "\n",
    "    def add(self,obs,next_obs,action):\n",
    "        self.episode['observation'].append(np.array(obs['image_observation']))\n",
    "        self.episode['achieved_goal'].append(np.array(obs['achieved_goal']))\n",
    "        self.episode['achieved_goal_image'].append(np.array(obs['image_observation']))\n",
    "\n",
    "        self.episode['achieved_goal_image_next'].append(np.array(next_obs['image_observation']))\n",
    "        self.episode['desired_goal'].append(np.array(obs['desired_goal']))\n",
    "        self.episode['desired_goal_image'].append(np.array(obs['desired_goal_image']))\n",
    "        self.episode['action'].append(np.array(action))\n",
    "        \n",
    "        if len(self.episode) ==  self.T:\n",
    "            self.store_episode([np.array(self.episode['observation']),self.episode['achieved_goal'],\n",
    "                                self.episode['achieved_goal_image_next'],self.episode['desired_goal'],\n",
    "                                self.episode['desired_goal_image'],self.episode['action']])\n",
    "            self.episode = defaultdict(list)\n",
    "\n",
    "    def store_episode(self, episode_batch):\n",
    "        observation,achieved_goal, achieved_goal_image, desired_goal,desired_goal_image,actions = episode_batch\n",
    "        batch_size = observation.shape[0]\n",
    "        # with self.lock:\n",
    "        idxs = self._get_storage_idx(inc=batch_size)\n",
    "        # store the informations\n",
    "        self.buffers['achieved_goal'][idxs] = achieved_goal\n",
    "        self.buffers['achieved_goal_image'][idxs] = achieved_goal_image\n",
    "        self.buffers['achieved_goal_image_next'][idxs] = achieved_goal_image\n",
    "        self.buffers['desired_goal'][idxs] = desired_goal\n",
    "        self.buffers['desired_goal_image'][idxs] = desired_goal_image\n",
    "        self.buffers['actions'][idxs] = actions\n",
    "        self.n_transitions_stored += self.T * batch_size\n",
    "\n",
    "    def _get_storage_idx(self, inc=None):\n",
    "        inc = inc or 1\n",
    "        if self.current_size+inc <= self.size:\n",
    "            idx = np.arange(self.current_size, self.current_size+inc)\n",
    "        elif self.current_size < self.size:\n",
    "            overflow = inc - (self.size - self.current_size)\n",
    "            idx_a = np.arange(self.current_size, self.size)\n",
    "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
    "            idx = np.concatenate([idx_a, idx_b])\n",
    "        else:\n",
    "            idx = np.random.randint(0, self.size, inc)\n",
    "        self.current_size = min(self.size, self.current_size+inc)\n",
    "        if inc == 1:\n",
    "            idx = idx[0]\n",
    "        return idx\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HERSampler:\n",
    "    def __init__(self, replay_strategy, replay_k, reward_func=None):\n",
    "        self.replay_strategy= replay_strategy\n",
    "        self.replay_k= replay_k # Number of virtual transitions to create per real transition, by sampling new goals.\n",
    "        # self.image_encoder= image_encoder \n",
    "        \n",
    "        if self.replay_strategy == 'future':\n",
    "            self.future_p= 1 - (1. / (1 + replay_k))\n",
    "        else:\n",
    "            self.future_p = 0\n",
    "\n",
    "        self.reward_func = reward_func\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "    \n",
    "    \n",
    "    def sample_her_transitions(self, episode_batch, batch_size_in_transitions ):\n",
    "        T = episode_batch['actions'].shape[1]\n",
    "        rollout_batch_size= episode_batch['actions'].shape[0]\n",
    "        batch_size = batch_size_in_transitions\n",
    "        # print(rollout_batch_size,batch_size,T)\n",
    "        # np.random.randint - low-inclusive and high-exclusive, hence max_env_steps+1 \n",
    "        episode_idxs = np.random.randint(0, rollout_batch_size, batch_size)\n",
    "        t_samples = np.random.randint(T, size=batch_size)\n",
    "        transitions = {key: episode_batch[key][episode_idxs, t_samples].copy() for key in episode_batch.keys()}\n",
    "        \n",
    "          # her idx\n",
    "        her_indexes = np.where(np.random.uniform(size=batch_size) < self.future_p)\n",
    "        # TODO try future strategy with only last state? \n",
    "\n",
    "        # TODO current strategy randomly samples index from the her_index till last \n",
    "        future_offset = np.random.uniform(size=batch_size) * (T - t_samples)\n",
    "        future_offset = future_offset.astype(int)\n",
    "\n",
    "        \n",
    "        future_t = (t_samples + 1 + future_offset)[her_indexes]\n",
    "        # replace go with achieved goal\n",
    "        future_ag_image = episode_batch['achieved_goal_image'][episode_idxs[her_indexes], future_t]\n",
    "        transitions['desired_goal_image'][her_indexes] = future_ag_image\n",
    "        # to get the params to re-compute reward \n",
    "        # image based dense reward compute \n",
    "        transitions['reward'] = np.expand_dims(self.reward_func(transitions['achieved_goal_image_next'], transitions['desired_goal_image'],image_based=True), 1)\n",
    "        transitions = {k: transitions[k].reshape(batch_size, *transitions[k].shape[1:]) for k in transitions.keys()}\n",
    "\n",
    "        return transitions\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 84, 84)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,) + env.observation_space['image_observation'].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customherreplay():\n",
    "    def __init__(self,observation_space, action_space,max_episode_stored, max_episode_length,goal_selection_strategy=\"future\"):\n",
    "        self.goal_selection_strategy = goal_selection_strategy\n",
    "        action_dim = action_space.shape\n",
    "        self.obs_shape = observation_space['image_observation'].shape\n",
    "        self.goal_shape = observation_space['desired_goal_image'].shape\n",
    "        self.max_episode_length = max_episode_length\n",
    "        self.max_episode_stored = max_episode_stored\n",
    "        input_shape = {\n",
    "                \"observation\": (1,) + self.obs_shape,\n",
    "                \"achieved_goal\": (1,) + self.goal_shape,\n",
    "                \"desired_goal\": (1,) + self.goal_shape,\n",
    "                \"action\": (action_dim,),\n",
    "                \"reward\": (1,),\n",
    "                \"next_obs\": (1,) + self.obs_shape,\n",
    "                \"next_achieved_goal\": (1,) + self.goal_shape,\n",
    "                \"next_desired_goal\": (1,) + self.goal_shape,\n",
    "                \"done\": (1,),\n",
    "            }\n",
    "            self._observation_keys = [\"observation\", \"achieved_goal\", \"desired_goal\"]\n",
    "            self._buffer = {\n",
    "                key: np.zeros((self.max_episode_stored, self.max_episode_length, *dim), dtype=np.float32)\n",
    "                for key, dim in input_shape.items()\n",
    "            }\n",
    "            # Store info dicts are it can be used to compute the reward (e.g. continuity cost)\n",
    "            self.info_buffer = [deque(maxlen=self.max_episode_length) for _ in range(self.max_episode_stored)]\n",
    "            # episode length storage, needed for episodes which has less steps than the maximum length\n",
    "            self.episode_lengths = np.zeros(self.max_episode_stored, dtype=np.int64)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def sample_goals(\n",
    "        self,\n",
    "        episode_indices: np.ndarray,\n",
    "        her_indices: np.ndarray,\n",
    "        transitions_indices: np.ndarray,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sample goals based on goal_selection_strategy.\n",
    "        This is a vectorized (fast) version.\n",
    "        :param episode_indices: Episode indices to use.\n",
    "        :param her_indices: HER indices.\n",
    "        :param transitions_indices: Transition indices to use.\n",
    "        :return: Return sampled goals.\n",
    "        \"\"\"\n",
    "        her_episode_indices = episode_indices[her_indices]\n",
    "\n",
    "        if self.goal_selection_strategy == GoalSelectionStrategy.FINAL:\n",
    "            # replay with final state of current episode\n",
    "            transitions_indices = self.episode_lengths[her_episode_indices] - 1\n",
    "\n",
    "        elif self.goal_selection_strategy == GoalSelectionStrategy.FUTURE:\n",
    "            # replay with random state which comes from the same episode and was observed after current transition\n",
    "            transitions_indices = np.random.randint(\n",
    "                transitions_indices[her_indices] + 1, self.episode_lengths[her_episode_indices]\n",
    "            )\n",
    "\n",
    "        elif self.goal_selection_strategy == GoalSelectionStrategy.EPISODE:\n",
    "            # replay with random state which comes from the same episode as current transition\n",
    "            transitions_indices = np.random.randint(self.episode_lengths[her_episode_indices])\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Strategy {self.goal_selection_strategy} for sampling goals not supported!\")\n",
    "\n",
    "        return self._buffer[\"achieved_goal\"][her_episode_indices, transitions_indices]\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _sample_transitions(\n",
    "        self,\n",
    "        batch_size: Optional[int],\n",
    "        maybe_vec_env: Optional[VecNormalize],\n",
    "        online_sampling: bool,\n",
    "        n_sampled_goal: Optional[int] = None,\n",
    "    ) -> Union[DictReplayBufferSamples, Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray], np.ndarray, np.ndarray]]:\n",
    "        \"\"\"\n",
    "        :param batch_size: Number of element to sample (only used for online sampling)\n",
    "        :param env: associated gym VecEnv to normalize the observations/rewards\n",
    "            Only valid when using online sampling\n",
    "        :param online_sampling: Using online_sampling for HER or not.\n",
    "        :param n_sampled_goal: Number of sampled goals for replay. (offline sampling)\n",
    "        :return: Samples.\n",
    "        \"\"\"\n",
    "        # Select which episodes to use\n",
    "       \n",
    "        assert batch_size is not None, \"No batch_size specified for online sampling of HER transitions\"\n",
    "        # Do not sample the episode with index `self.pos` as the episode is invalid\n",
    "        if self.full:\n",
    "            episode_indices = (\n",
    "                np.random.randint(1, self.n_episodes_stored, batch_size) + self.pos\n",
    "            ) % self.n_episodes_stored\n",
    "        else:\n",
    "            episode_indices = np.random.randint(0, self.n_episodes_stored, batch_size)\n",
    "        # A subset of the transitions will be relabeled using HER algorithm\n",
    "        her_indices = np.arange(batch_size)[: int(self.her_ratio * batch_size)]\n",
    "        \n",
    "        ep_lengths = self.episode_lengths[episode_indices]\n",
    "\n",
    "        # Special case when using the \"future\" goal sampling strategy\n",
    "        # we cannot sample all transitions, we have to remove the last timestep\n",
    "        if self.goal_selection_strategy == GoalSelectionStrategy.FUTURE:\n",
    "            # restrict the sampling domain when ep_lengths > 1\n",
    "            # otherwise filter out the indices\n",
    "            her_indices = her_indices[ep_lengths[her_indices] > 1]\n",
    "            ep_lengths[her_indices] -= 1\n",
    "\n",
    "       \n",
    "        # Select which transitions to use\n",
    "        transitions_indices = np.random.randint(ep_lengths)\n",
    "      \n",
    "        # get selected transitions\n",
    "        transitions = {key: self._buffer[key][episode_indices, transitions_indices].copy() for key in self._buffer.keys()}\n",
    "\n",
    "        # sample new desired goals and relabel the transitions\n",
    "        new_goals = self.sample_goals(episode_indices, her_indices, transitions_indices)\n",
    "        transitions[\"desired_goal\"][her_indices] = new_goals\n",
    "\n",
    "        # Convert info buffer to numpy array\n",
    "        transitions[\"info\"] = np.array(\n",
    "            [\n",
    "                self.info_buffer[episode_idx][transition_idx]\n",
    "                for episode_idx, transition_idx in zip(episode_indices, transitions_indices)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Edge case: episode of one timesteps with the future strategy\n",
    "        # no virtual transition can be created\n",
    "        if len(her_indices) > 0:\n",
    "            # Vectorized computation of the new reward\n",
    "            transitions[\"reward\"][her_indices, 0] = self.reward_func(\n",
    "               \n",
    "                # the new state depends on the previous state and action\n",
    "                # s_{t+1} = f(s_t, a_t)\n",
    "                # so the next_achieved_goal depends also on the previous state and action\n",
    "                # because we are in a GoalEnv:\n",
    "                # r_t = reward(s_t, a_t) = reward(next_achieved_goal, desired_goal)\n",
    "                # therefore we have to use \"next_achieved_goal\" and not \"achieved_goal\"\n",
    "                transitions[\"next_achieved_goal\"][her_indices, 0],\n",
    "                # here we use the new desired goal\n",
    "                transitions[\"desired_goal\"][her_indices, 0],\n",
    "               \n",
    "            )\n",
    "\n",
    "        # concatenate observation with (desired) goal\n",
    "        observations = self._normalize_obs(transitions, maybe_vec_env)\n",
    "\n",
    "        # HACK to make normalize obs and `add()` work with the next observation\n",
    "        next_observations = {\n",
    "            \"observation\": transitions[\"next_obs\"],\n",
    "            \"achieved_goal\": transitions[\"next_achieved_goal\"],\n",
    "            # The desired goal for the next observation must be the same as the previous one\n",
    "            \"desired_goal\": transitions[\"desired_goal\"],\n",
    "        }\n",
    "        next_observations = self._normalize_obs(next_observations, maybe_vec_env)\n",
    "\n",
    "        if online_sampling:\n",
    "            next_obs = {key: self.to_torch(next_observations[key][:, 0, :]) for key in self._observation_keys}\n",
    "\n",
    "            normalized_obs = {key: self.to_torch(observations[key][:, 0, :]) for key in self._observation_keys}\n",
    "\n",
    "            return [observations=normalized_obs,\n",
    "                actions=self.to_torch(transitions[\"action\"]),\n",
    "                next_observations=next_obs,\n",
    "                dones=self.to_torch(transitions[\"done\"]),\n",
    "                rewards=self.to_torch((transitions[\"reward\"]))]\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            return observations, next_observations, transitions[\"action\"], transitions[\"reward\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "\n",
    "class GoalSelectionStrategy(Enum):\n",
    "    \"\"\"\n",
    "    The strategies for selecting new goals when\n",
    "    creating artificial transitions.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select a goal that was achieved\n",
    "    # after the current step, in the same episode\n",
    "    FUTURE = 0\n",
    "    # Select the goal that was achieved\n",
    "    # at the end of the episode\n",
    "    FINAL = 1\n",
    "    # Select a goal that was achieved in the episode\n",
    "    EPISODE = 2\n",
    "\n",
    "\n",
    "# For convenience\n",
    "# that way, we can use string to select a strategy\n",
    "KEY_TO_GOAL_STRATEGY = {\n",
    "    \"future\": GoalSelectionStrategy.FUTURE,\n",
    "    \"final\": GoalSelectionStrategy.FINAL,\n",
    "    \"episode\": GoalSelectionStrategy.EPISODE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
